{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Preprocessing administrative data\n",
    "\n",
    "This is the first step in the computation. Running this notebook requires you to contact Statistics Denmark for data on population per square. Moreover, you also need to generate the grid data. \n",
    "\n",
    "Throughout we use various packages for network and spatial data. Some packages are not included in the standard Anaconda installer. E.g. with running the following line (assuming you are using a new `anaconda` from spring 2020).\n",
    "\n",
    "`conda install -c conda-forge geopandas python-louvain -y`\n",
    "\n",
    "### Before you proceed\n",
    "\n",
    "If you do not have data gridded data you should either contact Statistics Denmark or use dummy data (Step 0). Note that you can change the list of municipalities, `dummy_mun_codes`, as well as and change the `dummy_compute` variable in the [config file](sqr/core/config.py). \n",
    "\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import fiona\n",
    "\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "from sqr.core.shape import make_gdf_square_data, find_neighbor_shapes\n",
    "from sqr.core.shape import label2coord, polygon_from_north_east, make_spatial_grid\n",
    "from sqr.pre_assign import assign_cells_partition, merge_insufficient, pre_partition_area\n",
    "from sqr.miscellaneous import read_parse_mun\n",
    "from sqr.core.config import years, cell_label\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "\n",
    "Generate square net grid data for each municipality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding you need to either contact Statistics Denmark (DST) about getting access to square net data or generate dummy dummy (see step 0). \n",
    "\n",
    "Load kvadratnet with population data into HDF store: 'data/parsed/kvadrat_data.hdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/parsed/', exist_ok=True)\n",
    "\n",
    "# file paths for count data by square net cells\n",
    "persons_path = 'data/raw/ind_dummy.csv'\n",
    "households_path = 'data/raw/hh_dummy.csv'\n",
    "\n",
    "reparse_pop_data = False\n",
    "if reparse_pop_data:\n",
    "\n",
    "    pers_df = pd.read_csv(persons_path)\n",
    "    pers_df.to_hdf('data/parsed/KN100m_pop.hdf', key = 'pers', mode='w')\n",
    "\n",
    "    hh_df = pd.read_csv(households_path)\n",
    "    hh_df.to_hdf('data/parsed/KN100m_pop.hdf', key = 'hh', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign square to municipality shapes\n",
    "\n",
    "### Load data\n",
    "Load square cell population data  (for persons AND households) to perform various calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "zeros_joint = {}\n",
    "for label in ('pers', 'hh'):\n",
    "\n",
    "    df = pd.read_hdf('data/parsed/KN100m_pop.hdf', key=label).fillna(0)\n",
    "    df['minimum'] = df[years].min(axis=1)\n",
    "    df['mean'] = df[years].mean(axis=1)\n",
    "    \n",
    "    dfs[label] = df\n",
    "    \n",
    "    \n",
    "# compute overlap cells and begin computation for finding\n",
    "# years where cells have new construction   \n",
    "joint_cells = np.intersect1d(dfs['pers'][cell_label], dfs['hh'][cell_label])    \n",
    "for label in ('pers', 'hh'):    \n",
    "    dfs[label] = dfs[label]\\\n",
    "                    .set_index(cell_label)\\\n",
    "                    .reindex(joint_cells)\\\n",
    "                    .fillna(0)\n",
    "          \n",
    "    zeros_joint[label] = \\\n",
    "        dfs[label]\\\n",
    "            .loc[:,years]\\\n",
    "            .pipe(lambda df: (df==0).values.reshape(df.shape[0],df.shape[1],1))\n",
    "\n",
    "zeros_all = \\\n",
    "    pd.DataFrame(\n",
    "        data= np.concatenate([zeros_joint['pers'],\n",
    "                              zeros_joint['hh']],\n",
    "                             axis=2)\\\n",
    "                .min(axis=2),\n",
    "        index=joint_cells,\n",
    "        columns=years)\n",
    "\n",
    "\n",
    "zeros = zeros_all[zeros_all.max(1)]\n",
    "zeros_t = zeros.T\n",
    "\n",
    "inhabit = (zeros_t.shift(1).fillna(False) & (~zeros_t))\n",
    "inhabit_single = (inhabit.sum(axis=0)==1) & (~zeros.iloc[:,-1])\n",
    "inhabit_year = zeros_t\\\n",
    "                .loc[:,inhabit_single]\\\n",
    "                .idxmin()\\\n",
    "                .rename('inhabit_year')\\\n",
    "                .astype(int)\n",
    "\n",
    "# merge into single dataframe\n",
    "pop = pd.concat(dfs,axis=1)\n",
    "pop.columns = ['_'.join(reversed(c)) for c in  pop.columns.tolist()]\n",
    "pop = pop.join(inhabit_year).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load square cell information and geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqr.core.shape import make_spatial_grid \n",
    "\n",
    "if not os.path.exists('data/parsed/sqr_info.geojson'):\n",
    "    gdf_kommuner = read_parse_mun(compute_stats=False)\n",
    "\n",
    "    # compute grid\n",
    "    coords = \\\n",
    "        gdf_kommuner\\\n",
    "        .groupby('KOMNAVN')\\\n",
    "        .apply(make_spatial_grid)\\\n",
    "        .drop_duplicates()\\\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    # centroid coordinate\n",
    "    coords['e_cent'] = (coords.e.astype(int)*100+50).astype(np.int32)    \n",
    "    coords['n_cent'] = (coords.n.astype(int)*100+50).astype(np.int32)\n",
    "\n",
    "    p1 = pyproj.Proj(fiona.crs.from_epsg(25832))\n",
    "    p2 = pyproj.Proj(fiona.crs.from_epsg(4326))    \n",
    "\n",
    "    gps_coords = pyproj.transform(p1, p2, coords.e_cent.values, coords.n_cent.values)\n",
    "    gps_coords = pd.DataFrame(np.array(gps_coords).T, columns = ['lon_cent','lat_cent'])\n",
    "\n",
    "    coords.join(gps_coords).to_file('data/parsed/sqr_info.geojson', driver='GeoJSON')\n",
    "    \n",
    "all_gdf = gpd.read_file('data/parsed/sqr_info.geojson')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign square cells to municipalities (municipalities after kommunalreformen 2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_kommuner = read_parse_mun()\n",
    "\n",
    "assignments = assign_cells_partition(gdf_kommuner, all_gdf)\n",
    "\n",
    "assignment_dict = assignments\\\n",
    "                    .groupby('assignment')\\\n",
    "                    .apply(lambda g: g.index.tolist())\\\n",
    "                    .to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in assignment_dict.keys():\n",
    "    assignment_idxs = assignment_dict[idx]\n",
    "    \n",
    "    out_df = all_gdf\\\n",
    "            .loc[assignment_idxs]\\\n",
    "            .drop('geometry', axis=1)\\\n",
    "            .reset_index()\\\n",
    "            .rename(columns={'index':'square_idx'})\n",
    "    \n",
    "    out_df = out_df.merge(pop, \n",
    "                          right_on=cell_label,\n",
    "                          left_on=cell_label, \n",
    "                          how='left')\n",
    "    \n",
    "    pd.DataFrame(out_df).to_hdf('data/parsed/sqr_mun.hdf', key='sqidx%i'% idx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check no cells is overlapping for ANY pair of municipality indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "mun_adjacent = find_neighbor_shapes(gdf_kommuner)[['idx1', 'idx2']].values\n",
    "\n",
    "for (i1,i2) in mun_adjacent:    \n",
    "    if np.intersect1d(assignment_dict[i1], assignment_dict[i2]).size>0:\n",
    "        errors+= [(i1,i2)]\n",
    "        \n",
    "print (errors)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition municipality shapes into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_kommuner  = read_parse_mun()\n",
    "\n",
    "select =  gdf_kommuner[(gdf_kommuner.to_assign) & (gdf_kommuner.cell_count>25000)]\n",
    "\n",
    "for idx in tqdm(select.index.tolist()):\n",
    "    \n",
    "    origin_geom= gdf_kommuner.loc[idx].geometry\n",
    "    mun_df = pd.read_hdf('data/parsed/sqr_mun.hdf', 'sqidx%i' % idx)\n",
    "    mun_gdf = make_gdf_square_data(mun_df)\n",
    "    pre_part = pre_partition_area(mun_gdf, origin_geom)\n",
    "    \n",
    "    pre_part_suff = merge_insufficient(pre_part)\n",
    "    \n",
    "    assignment = assign_cells_partition(pre_part, mun_gdf)\n",
    "\n",
    "    mun_df = mun_df\\\n",
    "                .join(assignment)\\\n",
    "                .drop('geometry', axis=1)\n",
    "\n",
    "    for sub_idx, sub_df in mun_df.groupby('assignment'):\n",
    "        out_key = 'sqidx%i_%i' % (idx, sub_idx)\n",
    "        sub_df.to_hdf('data/parsed/sqr_mun_sub.hdf', key = out_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check plot subpartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mun_code = '179' # select Bornholm\n",
    "\n",
    "hdf = pd.HDFStore('data/parsed/sqr_mun_sub.hdf')\n",
    "keys = [k for k in hdf.keys() if f'{mun_code}_' in k]    \n",
    "hdf.close()\n",
    "mun_df = pd.concat(pd.read_hdf('data/parsed/sqr_mun_sub.hdf', key = k) for k in keys)\n",
    "mun_gdf = make_gdf_square_data(mun_df)\n",
    "\n",
    "mun_gdf\\\n",
    "    .groupby('assignment')\\\n",
    "    .apply(lambda df: df.unary_union)\\\n",
    "    .pipe(lambda s: gpd.GeoDataFrame(s.index, geometry=gpd.GeoSeries(s)))\\\n",
    "    .plot(column='assignment')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "privacy_spatial",
   "language": "python",
   "name": "privacy_spatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
