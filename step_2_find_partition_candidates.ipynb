{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Compute partition candidates\n",
    "\n",
    "This is the second step in the computation. We will try to use the pre-partitioned shapes of municipalities to find feasible candidates of cell partitions. It is required to complete STEP 0 before beginning.\n",
    "\n",
    "\n",
    "\n",
    "# Packages\n",
    "\n",
    "Load modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "\n",
    "from sqr.core.config import years, years_hh, years_pers, cell_label, minimum_cols\n",
    "from sqr.core.network import local_graph\n",
    "from sqr.core.scoring import partition_score\n",
    "from sqr.core.shape import make_gdf_square_data\n",
    "from sqr.main_assign import get_assignment, data_cols\n",
    "from sqr.miscellaneous import read_parse_mun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input\n",
    "\n",
    "The configuration of how to run the assignment is set below. Subsequently a list of jobs with inputs for making the partitions is created.\n",
    "\n",
    "Recall that some municipalities are split subparts. As a consequence we run procedure for the municiptal level (i.e., 'mun') but also at the sub-municipality level (i.e. 'submun') for municipalities with larger areas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "gdf_kommuner = read_parse_mun() \n",
    "\n",
    "# parameters\n",
    "trade = True\n",
    "num_iter = 4\n",
    "\n",
    "# init job list\n",
    "job_list = []\n",
    "\n",
    "# get input/output references\n",
    "for level in ('mun', 'submun'):\n",
    "    if level == 'mun':\n",
    "        in_file = 'data/parsed/sqr_mun.hdf' \n",
    "        out_file = 'data/candidates.hdf'\n",
    "        selection = (gdf_kommuner.to_assign) & (gdf_kommuner.cell_count<5000) \n",
    "        mun_indices = gdf_kommuner[selection].index.tolist()\n",
    "\n",
    "    elif level == 'submun':    \n",
    "        in_file = 'data/parsed/sqr_mun_sub.hdf'\n",
    "        out_file = 'data/candidates_sub.hdf'\n",
    "        selection = (gdf_kommuner.to_assign) & (gdf_kommuner.cell_count>=5000) \n",
    "        mun_indices = gdf_kommuner[selection].index.tolist()\n",
    "    \n",
    "    \n",
    "    # fetch keys\n",
    "    datastore = pd.HDFStore(in_file)\n",
    "    datakeys = datastore.keys()    \n",
    "    keys = pd.DataFrame(data = [(k.split('_')[0][6:], k[6:], k[1:]) for k in datakeys], \n",
    "                        columns = ['mun_idx','idx','key'])    \n",
    "    keys = keys[keys.mun_idx.astype(int).isin(mun_indices)]\n",
    "    datastore.close()\n",
    "    \n",
    "    \n",
    "    mun_pop = {'pers':{}, 'hh':{}}\n",
    "    mun_cell_count = {}\n",
    "    \n",
    "    # fill up job list\n",
    "    # load input for processing\n",
    "    for (i,row) in keys.iterrows():\n",
    "        df = pd.read_hdf(in_file, key=row.key)        \n",
    "\n",
    "        if df.shape[0]>1:\n",
    "            try:\n",
    "                accomplished = pd.read_hdf(out_file, key='munidx%s' % row.idx).shape[0]\n",
    "                remain_num_iter = max(0, num_iter - accomplished)\n",
    "\n",
    "            except:\n",
    "                remain_num_iter = num_iter\n",
    "\n",
    "            if remain_num_iter > 0:\n",
    "                G = local_graph(df)\n",
    "                big_G = local_graph(df, max_dist=3)\n",
    "\n",
    "\n",
    "                pers_density = df[years_pers].fillna(0).mean().mean()\n",
    "                pers_count = df[years_pers].sum(0).min()\n",
    "                hh_density = df[years_hh].fillna(0).mean().mean()\n",
    "                hh_count = df[years_hh].sum(0).min()\n",
    "\n",
    "                mun_pop['pers'][row.idx] = pers_count\n",
    "                mun_pop['hh'][row.idx] = hh_count\n",
    "                mun_cell_count[row.idx] = df.shape[0]\n",
    "\n",
    "                if (pers_count>=100)&(hh_count>=50):                \n",
    "                    job_list += [(row.idx,df,G,big_G,pers_density,hh_density,trade) \n",
    "                                 for _ in range(remain_num_iter)\n",
    "                                ]\n",
    "\n",
    "np.random.shuffle(job_list)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run partition algorithm\n",
    "Below we run the algorithm  for partitioning the municipal data. There are two options - use a single core for computation or parallelizing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output folder\n",
    "os.makedirs('data/temp_output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single core computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(job_list[:])):\n",
    "#     idx = job_list[i][0].split('_')[0]\n",
    "#     print(gdf_kommuner.KOMNAVN[int(idx)])\n",
    "#     get_assignment(job_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi core computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "proc = Parallel(n_jobs=2)(delayed(get_assignment)(job) for job in job_list[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse temporary files\n",
    "Check for available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir('data/temp_output')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_loader(f):\n",
    "    try:\n",
    "        return pd.read_csv('data/temp_output/%s' % f)\n",
    "    except:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179_18 data/candidates_sub.hdf\n",
      "179_19 data/candidates_sub.hdf\n",
      "179_20 data/candidates_sub.hdf\n",
      "179_8 data/candidates_sub.hdf\n",
      "5 files processed\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('data/temp_output')\n",
    "\n",
    "if len(files)>0:    \n",
    "    \n",
    "    output = pd.concat([file_loader(f) for f in files], axis=0)\n",
    "\n",
    "    by_mun_idx = output.groupby('mun_idx')\n",
    "    \n",
    "    old_keys = []\n",
    "    for out_file_name in ('candidates', 'candidates_sub'):    \n",
    "        out_file = f'data/{out_file_name}.hdf'\n",
    "        datastore = pd.HDFStore(out_file)\n",
    "        old_keys += [k[1:] for k in datastore.keys()]\n",
    "        datastore.close()\n",
    "\n",
    "    for idx, df_input in by_mun_idx:                \n",
    "        out_file = 'data/candidates_sub.hdf' if ('_' in str(idx)) else 'data/candidates.hdf'\n",
    "        print (idx, out_file)\n",
    "\n",
    "        if ('munidx%s' % idx) in old_keys:\n",
    "            existing = pd.read_hdf(out_file, key='munidx%s' % idx)\n",
    "            output = pd\\\n",
    "                    .concat([df_input, existing])\\\n",
    "                    .drop_duplicates(subset=['finish_ts'])\n",
    "\n",
    "        else:\n",
    "            output = df_input\n",
    "    \n",
    "        output.to_hdf(out_file, key='munidx%s' % idx)\n",
    "\n",
    "    print(len(files), 'files processed')\n",
    "    for f in files: \n",
    "        os.remove('data/temp_output/%s' % f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "privacy_spatial",
   "language": "python",
   "name": "privacy_spatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
