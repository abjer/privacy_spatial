{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Compute partition candidates\n",
    "\n",
    "This is the second step in the computation. We will try to use the pre-partitioned shapes of municipalities to find feasible candidates of cell partitions. It is required to complete STEP 0 before beginning.\n",
    "\n",
    "\n",
    "\n",
    "# Packages\n",
    "\n",
    "Setup multiprocessing module (note that cluster needs to be activated, see [here](https://ipyparallel.readthedocs.io/en/latest/process.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "dview = rc[:]\n",
    "\n",
    "number_of_engines = 42\n",
    "dview.map(os.chdir, [os.getcwd()]*number_of_engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load core modules across cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --local\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sqr.core.network import local_graph\n",
    "from sqr.main_assign import get_assignment, data_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load remaining modules for single core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "from sqr.core.scoring import partition_score\n",
    "from sqr.core.shape import make_gdf_square_data\n",
    "from sqr.miscellaneous import read_parse_mun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple engine approach\n",
    "\n",
    "### Settings\n",
    "\n",
    "The configuration of how to run the assignment is set below. \n",
    "\n",
    "Recall that some municipalities are split subparts. As a consequence we need to run procedure for the municiptal level (i.e., 'mun') but also at the sub-municipality level (i.e. 'submun') for municipalities with larger areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "kommuner = read_parse_mun()\n",
    "\n",
    "# main info\n",
    "level = 'submun'\n",
    "\n",
    "# additional info\n",
    "trade = True\n",
    "job_list = []\n",
    "num_iter = 1\n",
    "\n",
    "# get input/output references\n",
    "if level == 'mun':\n",
    "    in_file = 'data/parsed/sqr_mun.hdf' \n",
    "    out_file = 'data/candidates.hdf'\n",
    "    selection = (kommuner.to_assign) & (kommuner.cell_count<5000) \n",
    "    mun_indices = kommuner[selection].index.tolist()\n",
    "    \n",
    "elif level == 'submun':    \n",
    "    in_file = 'data/parsed/sqr_mun_sub.hdf'\n",
    "    out_file = 'data/candidates_sub.hdf'\n",
    "    selection = (kommuner.to_assign) & (kommuner.cell_count>=5000) \n",
    "    mun_indices = kommuner[selection].index.tolist()\n",
    "    \n",
    "else:\n",
    "    raise ValueError('Must specify level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data\n",
    "\n",
    "get references for input files and select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = pd.HDFStore(in_file)\n",
    "datakeys = datastore.keys()    \n",
    "keys = pd.DataFrame(data = [(k.split('_')[0][6:], k[6:], k[1:]) for k in datakeys], \n",
    "                    columns = ['mun_idx','idx','key'])    \n",
    "keys = keys[keys.mun_idx.astype(int).isin(mun_indices)]\n",
    "datastore.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load file info and make joblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mun_pop = {}\n",
    "mun_cell_count = {}\n",
    "\n",
    "years = list(map(str,range(1986,2016)))\n",
    "\n",
    "# load input for processing\n",
    "for (i,row) in keys.iterrows():\n",
    "    df = pd.read_hdf(in_file, key=row.key)        \n",
    "    \n",
    "    if df.shape[0]>1:\n",
    "        try:\n",
    "            accomplished = pd.read_hdf(out_file, key='munidx%s' % row.idx).shape[0]\n",
    "            remain_num_iter = max(0, num_iter - accomplished)\n",
    "            \n",
    "        except:\n",
    "            remain_num_iter = num_iter\n",
    "        \n",
    "        if remain_num_iter > 0:\n",
    "            G = local_graph(df)\n",
    "            big_G = local_graph(df, max_dist=3)\n",
    "            \n",
    "            pop_density = df[years].fillna(0).mean().mean()\n",
    "            pop_count = df[years].sum(0).min()\n",
    "\n",
    "            mun_pop[row.idx] = pop_count\n",
    "            mun_cell_count[row.idx] = df.shape[0]\n",
    "\n",
    "            if pop_count>=100:                \n",
    "                job_list += [(row.idx,df,G,big_G,pop_density,trade) for _ in range(remain_num_iter)]\n",
    "\n",
    "pd.np.random.shuffle(job_list)  \n",
    "print(len(job_list))\n",
    "\n",
    "# make output folder\n",
    "os.makedirs('data/temp_output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute joblist\n",
    "Single core computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_assignment(job_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi core computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.map_async(get_assignment, job_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse temporary files\n",
    "Check for available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('data/temp_output')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_loader(f):\n",
    "    try:\n",
    "        return pd.read_csv('data/temp_output/%s' % f)\n",
    "    except:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('data/temp_output')\n",
    "\n",
    "if len(files)>0:    \n",
    "    \n",
    "    output = pd.concat([file_loader(f) for f in files], axis=0)\n",
    "\n",
    "    by_mun_idx = output.groupby('mun_idx')\n",
    "    \n",
    "    datastore = pd.HDFStore(out_file)\n",
    "    old_keys = [k[1:] for k in datastore.keys()]\n",
    "    datastore.close()\n",
    "\n",
    "    for idx, df_input in by_mun_idx:        \n",
    "\n",
    "#         print (idx)\n",
    "        if ('munidx%s' % idx) in old_keys:\n",
    "            existing = pd.read_hdf(out_file, key='munidx%s' % idx)\n",
    "            output = pd\\\n",
    "                    .concat([df_input, existing])\\\n",
    "                    .drop_duplicates(subset=['finish_ts'])\n",
    "\n",
    "        else:\n",
    "            output = df_input\n",
    "    \n",
    "        output.to_hdf(out_file, key='munidx%s' % idx)\n",
    "\n",
    "    print(len(files))\n",
    "    for f in files: \n",
    "        os.remove('data/temp_output/%s' % f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostics\n",
    "Get output statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stat = []\n",
    "for idx in kommuner.index:\n",
    "    df = pd.read_hdf('data/parsed/sqr_mun.hdf', 'sqidx%i' % idx)\n",
    "    if df.minimum.sum()>0:\n",
    "        try:        \n",
    "            run_info = pd.read_hdf('data/candidates.hdf', key='munidx%i' % idx)\n",
    "            run_info_trade = run_info#[run_info.trade]\n",
    "            if run_info_trade.shape[0]>0:\n",
    "                output_stat.append([idx, run_info_trade.shape[0], run_info_trade.delta_t.median(),\n",
    "                                    df.minimum.shape[0], int(df.minimum.sum())])\n",
    "        except:\n",
    "            runs = 0\n",
    "#             output_stat.append([idx, runs, df.minimum.shape[0], int(df.minimum.sum())])\n",
    "\n",
    "output_stat = pd.DataFrame(output_stat, columns=['idx','run_count','run_time' ,'cell_count','pop_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# output_stat['oko5'] = output_stat.cell_count>3000\n",
    "\n",
    "fig = sns.lmplot(y='run_time',x='cell_count',order=2,data=output_stat)\n",
    "\n",
    "sns.plt.xlim(0,)\n",
    "sns.plt.ylim(0,)\n",
    "# fig.savefig('runtime_cellcount.pdf')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
