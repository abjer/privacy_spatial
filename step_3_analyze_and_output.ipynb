{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Analyze candidates and select optimal partition\n",
    "\n",
    "This is the final step where the output partitions are collected. From the collected partitions the ones that score best in terms of highest spatial precision and fewest missing values are selected.\n",
    "\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "# from itertools import chain\n",
    "\n",
    "from sqr.core.network import partition_surjective\n",
    "from sqr.core.scoring import partition_score, get_undominated\n",
    "from sqr.core.shape import  make_gdf_square_data\n",
    "from sqr.miscellaneous import read_parse_mun\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data \n",
    "\n",
    "#### Municipal data \n",
    "\n",
    "This data consists of the shape files for municipalities and muncipality data parsed to have square net information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kommuner = read_parse_mun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed data \n",
    "\n",
    "This data consists of the output from running 'Step 1: run assignments.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = 'data/temp_from_other/'\n",
    "main = 'data/'\n",
    "pareto = 'data/pareto/'\n",
    "\n",
    "othernames = {'candidates_sub.hdf':['candidates_sub_aws.hdf','candidates_sub_mac.hdf','candidates_sub_oko4.hdf'],\n",
    "              'candidates.hdf':['candidates_aws.hdf']}\n",
    "\n",
    "filenames = {'candidates.hdf':['pareto.hdf','parsed/sqr_mun.hdf'], \n",
    "             'candidates_sub.hdf':['pareto_sub.hdf','parsed/sqr_mun_sub.hdf']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge multiple sources (optional)\n",
    "If running the computation across multiple machines then various files can be merged. The code below may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attemp_read(in_file, input_key):\n",
    "#     try:\n",
    "#         return pd.read_hdf(in_file, key=input_key)\n",
    "#     except:\n",
    "#         return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list of files with output\n",
    "# candidate files = 'candidates.hdf', 'candidates_sub.hdf'\n",
    "\n",
    "# for filename in ['candidates.hdf']:\n",
    "    \n",
    "    \n",
    "#     out_file = main+filename\n",
    "    \n",
    "#     in_files = [other+f for f in othernames[filename]]\n",
    "#     input_keys = list(set(chain(*[pd.HDFStore(in_file).keys() for in_file in in_files])))\n",
    "\n",
    "\n",
    "#     for input_key in input_keys:    \n",
    "#         in_list = [attemp_read(f, input_key) for f in in_files]    \n",
    "#         input_df = pd.concat(in_list)\n",
    "        \n",
    "#         try:\n",
    "#             existing = pd.read_hdf(out_file, key=input_key)\n",
    "#             out = pd\\\n",
    "#                     .concat([input_df, existing])\\\n",
    "#                     .drop_duplicates(subset='finish_ts')\n",
    "#         except:\n",
    "#             out = input_df\n",
    "        \n",
    "#         out.to_hdf(out_file, key=input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select pareto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Pareto frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in ['candidates.hdf','candidates_sub.hdf']:\n",
    "    \n",
    "    in_file = main+filename\n",
    "    out_file = main+filenames[filename][0]\n",
    "    \n",
    "    datastore = pd.HDFStore(in_file)\n",
    "    datakeys = list(datastore.keys())\n",
    "    datastore.close()\n",
    "    \n",
    "    for input_key in tqdm(datakeys):\n",
    "        input_df = pd.read_hdf(in_file, key=input_key)\n",
    "        input_df = input_df.reset_index(drop=True)\n",
    "        \n",
    "        if input_df.shape[0]>1:\n",
    "            out = get_undominated(input_df)\n",
    "        else:\n",
    "            out = input_df\n",
    "        \n",
    "        out.to_hdf(out_file, key=input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose best partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(l):\n",
    "    return list(itertools.chain(*l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mun_pop = {}\n",
    "mun_indices = {}\n",
    "\n",
    "in_file = main+'candidates_sub.hdf'\n",
    "mun_file = main+'parsed/sqr_mun_sub.hdf'\n",
    "\n",
    "datastore = pd.HDFStore(in_file)\n",
    "datakeys = list(datastore.keys())\n",
    "datastore.close()\n",
    "\n",
    "for input_key in datakeys:\n",
    "    idx = input_key[7:]\n",
    "    input_df = pd.read_hdf(in_file, key=input_key)\n",
    "    mun_df = pd.read_hdf(mun_file, key='sqidx'+idx)\n",
    "\n",
    "    mun_pop[idx] = mun_df.minimum.sum()\n",
    "    mun_indices[idx] = mun_df.KN100mDK.to_dict()\n",
    "    \n",
    "\n",
    "mun_file = main+'parsed/sqr_mun.hdf'    \n",
    "datastore = pd.HDFStore(mun_file)\n",
    "datakeys = list(datastore.keys())\n",
    "datastore.close()\n",
    "    \n",
    "for input_key in datakeys:\n",
    "    idx = input_key[6:]\n",
    "    mun_df = pd.read_hdf(mun_file, key=input_key)\n",
    "    mun_pop[idx] = mun_df.minimum.sum()\n",
    "    mun_indices[idx] = mun_df.KN100mDK.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load optimal partition mun-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = main+'pareto.hdf'\n",
    "datastore = pd.HDFStore(in_file)\n",
    "datakeys = list(datastore.keys())\n",
    "datastore.close()\n",
    "\n",
    "\n",
    "pareto = pd.concat([pd.read_hdf(in_file, k) for k in datakeys]).reset_index()\n",
    "pareto.mun_idx = pareto.mun_idx.astype(int).astype(str)\n",
    "pareto['score'] = pareto.apply(lambda row: partition_score(row.weighted_dist, \n",
    "                                                           row.count_nonzero_null, \n",
    "                                                           mun_pop[row.mun_idx]), \n",
    "                               axis=1)\n",
    "optimals_idx = pareto.groupby('mun_idx').score.idxmax()\n",
    "optimals = pareto.loc[optimals_idx]\n",
    "optimals.partition = optimals.partition.apply(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load optimal partition sub mun-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = main+'pareto_sub.hdf'\n",
    "datastore = pd.HDFStore(in_file)\n",
    "datakeys = list(datastore.keys())\n",
    "datastore.close()\n",
    "\n",
    "\n",
    "pareto_sub = pd.concat([pd.read_hdf(in_file, k) for k in datakeys]).reset_index()\n",
    "pareto_sub['score'] = pareto_sub\\\n",
    "                        .apply(lambda row: partition_score(row.weighted_dist, \n",
    "                                                           row.count_nonzero_null, \n",
    "                                                           mun_pop[row.mun_idx]),axis=1)\n",
    "\n",
    "optimals_idx_sub = pareto_sub.groupby('mun_idx').score.idxmax()\n",
    "\n",
    "optimals_sub = pareto_sub.loc[optimals_idx_sub.values].copy()\n",
    "optimals_sub['pop_count'] = optimals_sub.mun_idx.apply(lambda m: mun_pop[m])\n",
    "optimals_sub['mun'] = optimals_sub.mun_idx.apply(lambda s: s.split('_')[0])\n",
    "optimals_sub.partition = optimals_sub.partition.apply(json.loads)\n",
    "\n",
    "# remove kÃ¸ge\n",
    "# optimals_sub = optimals_sub[~(optimals_sub['mun']=='163')]\n",
    "\n",
    "gb_idx = optimals_sub.groupby('mun')\n",
    "pop_sum = gb_idx.pop_count.sum()\n",
    "na_sum = gb_idx.pop_nonzero_null.sum()\n",
    "w_dist = gb_idx.apply(lambda g: (g.weighted_dist*g.pop_count).sum()/g.pop_count.sum())\n",
    "partitions = gb_idx.partition.apply(chain)\n",
    "\n",
    "\n",
    "optimals_sub_merg = pd.concat([na_sum,\n",
    "                               pop_sum,\n",
    "                               w_dist.rename('weighted_dist'),\n",
    "                               partitions.rename('partition')],axis=1)\n",
    "\n",
    "optimals_sub_merg['score'] = \\\n",
    "    optimals_sub_merg.apply(lambda row: partition_score(row.weighted_dist, \n",
    "                                                    row.pop_nonzero_null, \n",
    "                                                    row.pop_count),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['score','partition','pop_nonzero_null','weighted_dist']\n",
    "opt_mun = optimals.set_index('mun_idx')[merge_cols]\n",
    "opt_sub = optimals_sub_merg[merge_cols]                              \n",
    "\n",
    "merge = opt_mun.merge(opt_sub, how='outer', left_index=True,right_index=True).copy()\n",
    "merge[['score_x','score_y']] = merge[['score_x','score_y']].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(mun_idx, idx):\n",
    "    return mun_indices[mun_idx][idx]\n",
    "\n",
    "def get_labels(mun_idx, partition):\n",
    "    return [[get_label(mun_idx,i) for i in g] for g in partition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select maximal scoring partitoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_indices = merge.apply(lambda row: row.partition_x if row.score_x>row.score_y else row.partition_y, axis=1)\n",
    "\n",
    "optimal_labels =  optimal_indices\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={'index':'mun_idx',0:'partition'})\\\n",
    "                    .apply(lambda row: get_labels(str(row.mun_idx),row.partition),axis=1)\n",
    "\n",
    "optimal_labels.index = optimal_indices.index.to_series().astype(int)\n",
    "optimal_labels.sort_index(inplace=True)            \n",
    "            \n",
    "optimal_labels_one = list(itertools.chain(*optimal_labels.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output final partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/output_final/', exist_ok=True)\n",
    "\n",
    "with open ('data/output_final/partition.json', 'w') as f:\n",
    "    f.write(json.dumps(optimal_labels_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze optimal partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_miss = merge.apply(lambda row: row.pop_nonzero_null_x if row.score_x>row.score_y else row.pop_nonzero_null_y, axis=1)\n",
    "optimal_dist = merge.apply(lambda row: row.weighted_dist_x if row.score_x>row.score_y else row.weighted_dist_y, axis=1)\n",
    "\n",
    "optimal_stat = pd\\\n",
    "                .concat([optimal_miss,pd.Series(mun_pop),optimal_dist],axis=1)\\\n",
    "                .dropna()\\\n",
    "                .rename(columns={0:'miss',1:'total',2:'dist'})\n",
    "            \n",
    "optimal_stat.index = optimal_stat.index.to_series().astype(int)\n",
    "\n",
    "optimal_stat.sort_index().to_csv('data/output_final/statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check no duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_assigned = list(itertools.chain(*optimal_labels_one))\n",
    "is_assigned_ser = pd.Series(is_assigned)\n",
    "is_assigned_ser.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count population in partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers = pd.read_hdf('data/parsed/personer_celler.hdf', key='_100m').iloc[1:]\n",
    "\n",
    "years = list(map(str,range(1986,2016)))\n",
    "assignment = dict([(i,g_idx) for g_idx,g in enumerate(optimal_labels_one) for i in g])\n",
    "pers_assign = pers.set_index('ddkncelle100m').join(pd.Series(assignment,name='assignment'))\n",
    "pers_assign = pers_assign[pers_assign.assignment.notnull()]\n",
    "\n",
    "g = pers_assign.loc[optimal_labels_one[0]]\n",
    "distribution = pers_assign\\\n",
    "                .groupby('assignment')\\\n",
    "                .apply(lambda g: g[years].dropna(how='all',axis=1).sum(axis=0).min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify minimum population is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make histogram of population sizes in the final partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution.plot.hist(bins=100,log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count unassigned population for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_unassigned_years = pers[~pers.ddkncelle100m.isin(is_assigned)][years].fillna(0)\n",
    "pop_unassigned_years.sum(axis=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean annual cell size for cells not part of a partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_unassigned = pers[~pers.ddkncelle100m.isin(is_assigned)][years].fillna(0).mean(axis=1)\n",
    "pop_unassigned.plot.hist(bins=500,range=[0,100])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
